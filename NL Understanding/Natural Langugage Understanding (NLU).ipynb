{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063bf336",
   "metadata": {},
   "source": [
    "Natural Language Understanding (NLU) is the ability of a computer to understand human language as it is naturally spoken and written. Here are some applications of NLU:\n",
    "\n",
    "1. **Named Entity Recognition (NER)**:\n",
    "   Extract entities like names of people, organizations, locations, expressions of times, quantities, monetary values, etc. from the text.\n",
    "   Libraries: `spaCy`, `nltk`\n",
    "\n",
    "2. **Chatbots and Virtual Assistants**:\n",
    "   Understand user queries and generate appropriate responses.\n",
    "   Libraries: `Rasa`, `ChatterBot`\n",
    "\n",
    "3. **Part-of-Speech Tagging**:\n",
    "   Label words in a sentence with their respective parts of speech (e.g., noun, verb, adjective).\n",
    "   Libraries: `spaCy`, `nltk`\n",
    "\n",
    "4. **Text Classification**:\n",
    "   Classify the text into different categories such as spam detection, sentiment analysis, or topic categorization.\n",
    "   Libraries: `scikit-learn`, `TensorFlow`, `PyTorch`\n",
    "\n",
    "5. **Syntax Parsing**:\n",
    "   Analyze the grammatical structure of a sentence, establishing relationships between words.\n",
    "   Libraries: `spaCy`, `nltk`\n",
    "\n",
    "6. **Semantic Role Labeling**:\n",
    "   Determine the semantic relationships between words in a sentence (e.g., who did what to whom).\n",
    "   Libraries: `AllenNLP`\n",
    "\n",
    "7. **Machine Translation**:\n",
    "   Translate text from one language to another.\n",
    "   Libraries: `OpenNMT`, `Transformers` from Hugging Face\n",
    "\n",
    "8. **Coreference Resolution**:\n",
    "   Identify which words (pronouns and nouns) refer to the same object.\n",
    "   Libraries: `spaCy` (with neural coref plugin), `AllenNLP`\n",
    "\n",
    "9. **Text Summarization**:\n",
    "   Generate a concise and meaningful summary of a longer text.\n",
    "   Libraries: `Transformers` from Hugging Face, `Gensim`\n",
    "\n",
    "10. **Question Answering**:\n",
    "    Extract answers from a given text based on the posed question.\n",
    "    Libraries: `Transformers` from Hugging Face\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c015aae2",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdcbf6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2aa035ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "baad7ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have good spelling!\n"
     ]
    }
   ],
   "source": [
    "# Spelling correction\n",
    "blob = TextBlob(\"I havv good speling!\")\n",
    "print(blob.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4c73cd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"TextBlob is a great Python library.\"),\n",
       " Sentence(\"It makes text processing easy!\")]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization using Textblob\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"TextBlob is a great Python library. It makes text processing easy!\")\n",
    "blob.sentences  # Split into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "62b26818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['TextBlob', 'is', 'a', 'great', 'Python', 'library', 'It', 'makes', 'text', 'processing', 'easy'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.words      # Split into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e08e9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TextBlob', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('great', 'JJ'),\n",
       " ('Python', 'NNP'),\n",
       " ('library', 'NN'),\n",
       " ('It', 'PRP'),\n",
       " ('makes', 'VBZ'),\n",
       " ('text', 'JJ'),\n",
       " ('processing', 'VBG'),\n",
       " ('easy', 'JJ')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.tags #POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "af9cd355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['textblob', 'python', 'text processing'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fcf70cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"TextBlob is a great Python library. It makes text processing easy!\")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e62d5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    \"\"\"Return sentiment polarity and subjectivity of a given text using TextBlob.\"\"\"\n",
    "    analysis = TextBlob(text)\n",
    "    \n",
    "    # The polarity is a float value within the range [-1.0 to 1.0]\n",
    "    # where -1 indicates a negative sentiment and 1 indicates a positive sentiment.\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    \n",
    "    # Subjectivity is within the range [0.0 to 1.0] where 0.0 is very objective\n",
    "    # and 1.0 is very subjective.\n",
    "    subjectivity = analysis.sentiment.subjectivity\n",
    "    \n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "08353197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.6125, Subjectivity: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "text_sample = \"I love this product! It's amazing.\"\n",
    "polarity, subjectivity = analyze_sentiment(text_sample)\n",
    "\n",
    "print(f\"Polarity: {polarity}, Subjectivity: {subjectivity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b5d3414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with different samples\n",
    "samples = [\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Water boils at 100 degrees Celsius under standard atmospheric pressure.\",\n",
    "    \"I think chocolate ice cream is the best flavor ever.\",\n",
    "    \"The movie was incredibly boring and felt like a waste of time.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6069e031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Paris is the capital of France.'\n",
      "Polarity: 0.0, Subjectivity: 0.0\n",
      "\n",
      "Text: 'Water boils at 100 degrees Celsius under standard atmospheric pressure.'\n",
      "Polarity: 0.0, Subjectivity: 0.0\n",
      "\n",
      "Text: 'I think chocolate ice cream is the best flavor ever.'\n",
      "Polarity: 1.0, Subjectivity: 0.3\n",
      "\n",
      "Text: 'The movie was incredibly boring and felt like a waste of time.'\n",
      "Polarity: -0.6, Subjectivity: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in samples:\n",
    "    polarity, subjectivity = analyze_sentiment(sample)\n",
    "    print(f\"Text: '{sample}'\\nPolarity: {polarity}, Subjectivity: {subjectivity}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb2b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c331747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb4085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03b1f834",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c834ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U pywsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "24cf9253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "from pywsd.lesk import simple_lesk\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# # This is needed if you're using NLTK for the first time\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "617e9f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disambiguated sense: Synset('squash_racket.n.01')\n",
      "Definition: a small racket with a long handle used for playing squash\n"
     ]
    }
   ],
   "source": [
    "# Provide a sentence with the word 'bank' for disambiguation\n",
    "# sentence = \"I went to the bank to deposit my money.\"\n",
    "sentence = \"He picked up the bat and swung it\"\n",
    "\n",
    "# Using simple_lesk for WSD. It returns a synset.\n",
    "disambiguated_sense = simple_lesk(sentence, 'bat') # bank\n",
    "\n",
    "print(f\"Disambiguated sense: {disambiguated_sense}\")\n",
    "print(f\"Definition: {disambiguated_sense.definition()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56956ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d571ed8a",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c4429b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Prashant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Prashant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Downloading the Punkt Tokenizer and the averaged_perceptron_tagger (for POS tagging)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7881047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            POS       \n",
      "-------------------------\n",
      "The             DT        \n",
      "upcoming        JJ        \n",
      "India           NNP       \n",
      "vs              NN        \n",
      "Pakistan        NNP       \n",
      "match           NN        \n",
      "on              IN        \n",
      "Sunday          NNP       \n",
      "faces           VBZ       \n",
      "a               DT        \n",
      "significant     JJ        \n",
      "risk            NN        \n",
      "of              IN        \n",
      "being           VBG       \n",
      "interrupted     VBN       \n",
      "by              IN        \n",
      "rain            NN        \n",
      "again           RB        \n",
      ".               .         \n"
     ]
    }
   ],
   "source": [
    "text = \"The upcoming India vs Pakistan match on Sunday faces a significant risk of being interrupted by rain again.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "print(\"Word\".ljust(15), \"POS\".ljust(10))\n",
    "print(\"-\" * 25)\n",
    "for word, tag in pos_tags:\n",
    "    print(word.ljust(15), tag.ljust(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92efdabe",
   "metadata": {},
   "source": [
    "#### `nltk` uses the Penn Treebank POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9f402168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Prashant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'help' module is downloaded\n",
    "nltk.download('tagsets')\n",
    "\n",
    "# Displaying the Penn Treebank POS Tags and their explanations\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b548f",
   "metadata": {},
   "source": [
    "### POS Tagging using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70bb578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2958a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0b9eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, POS tagger, and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "079afa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The upcoming India vs Pakistan match on Sunday faces a significant risk of being interrupted by rain again.\"\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "57cbe302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4154921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            POS       \n",
      "-------------------------\n",
      "The             DET       \n",
      "upcoming        ADJ       \n",
      "India           PROPN     \n",
      "vs              ADP       \n",
      "Pakistan        PROPN     \n",
      "match           NOUN      \n",
      "on              ADP       \n",
      "Sunday          PROPN     \n",
      "faces           VERB      \n",
      "a               DET       \n",
      "significant     ADJ       \n",
      "risk            NOUN      \n",
      "of              ADP       \n",
      "being           AUX       \n",
      "interrupted     VERB      \n",
      "by              ADP       \n",
      "rain            NOUN      \n",
      "again           ADV       \n",
      ".               PUNCT     \n"
     ]
    }
   ],
   "source": [
    "print(\"Word\".ljust(15), \"POS\".ljust(10))\n",
    "print(\"-\" * 25)\n",
    "for token in doc:\n",
    "    print(token.text.ljust(15), token.pos_.ljust(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45deea3",
   "metadata": {},
   "source": [
    "**SpaCy uses the Universal POS Tags and its own fine-grained tags.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f98983d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Universal POS Tags in spaCy:\n",
      "--------------------------------------------------\n",
      "ADJ   : adjective\n",
      "ADP   : adposition\n",
      "ADV   : adverb\n",
      "AUX   : auxiliary\n",
      "CONJ  : conjunction\n",
      "CCONJ : coordinating conjunction\n",
      "DET   : determiner\n",
      "INTJ  : interjection\n",
      "NOUN  : noun\n",
      "NUM   : numeral\n",
      "PART  : particle\n",
      "PRON  : pronoun\n",
      "PROPN : proper noun\n",
      "PUNCT : punctuation\n",
      "SCONJ : subordinating conjunction\n",
      "SYM   : symbol\n",
      "VERB  : verb\n",
      "X     : other\n"
     ]
    }
   ],
   "source": [
    "# Extracting the POS tag map from the loaded language model\n",
    "pos_tags = nlp.get_pipe(\"tagger\").labels\n",
    "\n",
    "# Listing Universal POS Tags:\n",
    "print(\"\\nUniversal POS Tags in spaCy:\")\n",
    "print(\"-\" * 50)\n",
    "universal_tags = [\"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CONJ\", \"CCONJ\", \"DET\", \"INTJ\", \\\n",
    "                  \"NOUN\", \"NUM\", \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\"]\n",
    "\n",
    "for tag in universal_tags:\n",
    "    print(f\"{tag.ljust(6)}: {spacy.explain(tag)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "100d3544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$     : symbol, currency\n",
      "''    : closing quotation mark\n",
      ",     : punctuation mark, comma\n",
      "-LRB- : left round bracket\n",
      "-RRB- : right round bracket\n",
      ".     : punctuation mark, sentence closer\n",
      ":     : punctuation mark, colon or ellipsis\n",
      "ADD   : email\n",
      "AFX   : affix\n",
      "CC    : conjunction, coordinating\n",
      "CD    : cardinal number\n",
      "DT    : determiner\n",
      "EX    : existential there\n",
      "FW    : foreign word\n",
      "HYPH  : punctuation mark, hyphen\n",
      "IN    : conjunction, subordinating or preposition\n",
      "JJ    : adjective (English), other noun-modifier (Chinese)\n",
      "JJR   : adjective, comparative\n",
      "JJS   : adjective, superlative\n",
      "LS    : list item marker\n",
      "MD    : verb, modal auxiliary\n",
      "NFP   : superfluous punctuation\n",
      "NN    : noun, singular or mass\n",
      "NNP   : noun, proper singular\n",
      "NNPS  : noun, proper plural\n",
      "NNS   : noun, plural\n",
      "PDT   : predeterminer\n",
      "POS   : possessive ending\n",
      "PRP   : pronoun, personal\n",
      "PRP$  : pronoun, possessive\n",
      "RB    : adverb\n",
      "RBR   : adverb, comparative\n",
      "RBS   : adverb, superlative\n",
      "RP    : adverb, particle\n",
      "SYM   : symbol\n",
      "TO    : infinitival \"to\"\n",
      "UH    : interjection\n",
      "VB    : verb, base form\n",
      "VBD   : verb, past tense\n",
      "VBG   : verb, gerund or present participle\n",
      "VBN   : verb, past participle\n",
      "VBP   : verb, non-3rd person singular present\n",
      "VBZ   : verb, 3rd person singular present\n",
      "WDT   : wh-determiner\n",
      "WP    : wh-pronoun, personal\n",
      "WP$   : wh-pronoun, possessive\n",
      "WRB   : wh-adverb\n",
      "XX    : unknown\n",
      "_SP   : whitespace\n",
      "``    : opening quotation mark\n"
     ]
    }
   ],
   "source": [
    "# List of unique fine-grained tags in the loaded model\n",
    "tags = list(nlp.get_pipe(\"tagger\").labels)\n",
    "for tag in tags:\n",
    "    print(f\"{tag.ljust(5)} : {spacy.explain(tag)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b3837b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the German Corpus/Tokenizer/POS Tagger etc...\n",
    "# !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d5c138b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prashant\\anaconda3\\envs\\LLM\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'de_core_news_sm' (3.0.0) was trained with spaCy v3.0.0 and may not be 100% compatible with the current version (3.6.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Das', 'PRON'), ('ist', 'AUX'), ('ein', 'DET'), ('einfacher', 'ADJ'), ('Satz', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# Load the German language model\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def pos_tag_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "sentence = \"Das ist ein einfacher Satz.\"\n",
    "print(pos_tag_spacy(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c3d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed873204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d4f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35c6d18f",
   "metadata": {},
   "source": [
    "## Noun Phrase Chunking\n",
    "\n",
    "Chunking, often also referred to as noun phrase chunking, aims to extract phrases from unstructured text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "33e5dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "PM Modi, Macron hold lunch meet, eye India-France ties at new heights of progress.\n",
    "French president Emmanuel Macron said India did its utmost for the G-20 presidency to serve unity and peace.\n",
    "Prime Minister Narendra Modi on Sunday had lunch with French president Emmanuel Macron following the conclusion of the G20 Summit.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "939f5652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun Phrases using spaCy:\n",
      "--------------------------------------------------\n",
      "\n",
      "PM Modi\n",
      "Macron\n",
      "\n",
      "French\n",
      "Emmanuel Macron\n",
      "India\n",
      "\n",
      "Prime\n",
      "Minister Narendra Modi on Sunday\n",
      "Emmanuel Macron\n",
      "following the conclusion of the G20 Summit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "print(\"Noun Phrases using spaCy:\")\n",
    "print(\"-\" * 50)\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082061a",
   "metadata": {},
   "source": [
    "nltk uses a more manual approach with regular expressions to define the chunk grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d193598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "# Download necessary resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "18f14d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun Phrases using nltk:\n",
      "--------------------------------------------------\n",
      "the whimsical tale\n",
      "a young girl\n",
      "a rabbit\n",
      "hole\n",
      "a fantastical world\n",
      "a myriad\n",
      "bewildering adventure\n",
      "journey\n",
      "test\n",
      "logic\n",
      "reason\n",
      "the very nature\n",
      "reality\n",
      "a dream\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and POS Tag\n",
    "words = word_tokenize(text)\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Define chunk grammar for noun phrases\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# Parsing the sentence\n",
    "parser = RegexpParser(grammar)\n",
    "tree = parser.parse(pos_tags)\n",
    "\n",
    "print(\"Noun Phrases using nltk:\")\n",
    "print(\"-\" * 50)\n",
    "for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "    print(' '.join([word for word, tag in subtree.leaves()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55836d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fbd2e6d",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition (NER) is used to identify named entities such as persons, organizations, dates, etc., in the text. Let's extract named entities from the given text using both spaCy and nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ef6e1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Download necessary resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4440b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Indian Prime Minister Narendra Modi, French president Emmanuel Macron hold lunch meet, eye India-France ties at new heights of progress\n",
    "French president Emmanuel Macron said India did its utmost for the G-20 presidency to serve unity and peace.\n",
    "Prime Minister Narendra Modi on Sunday had lunch with French president Emmanuel Macron following the conclusion of the G20 Summit.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f1bfd2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"PM Modi handed over the G20 presidency to Brazil's Lula Da Silva, marking the end of the G20 Summit in Delhi. The Prime Minister handed over the ceremonial gavel to the Brazilian President to mark the transfer of the Presidency at the third G20 session \"One Future\" at the Bharat Mandapam in Delhi's Pragati Maidan. With this, the two-day mega conclave of world leaders under India's G20 Presidency in the national capital came to an end.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6d1cd4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\"Alice in Wonderland,\" penned by Lewis Carroll, narrates the whimsical tale of a young girl named Alice who tumbles down a rabbit hole and finds herself in a fantastical world filled with bizarre creatures and perplexing situations. As she navigates this Wonderland, she encounters a myriad of eccentric characters, including the Cheshire Cat, the Mad Hatter, the Queen of Hearts, and the White Rabbit, each of whom adds to her bewildering adventure. Throughout her journey, Alice faces challenges that test her logic and reason, ultimately leading her to question the very nature of reality before she finally awakens, realizing her adventures were but a dream.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "85f3d075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities using nltk:\n",
      "--------------------------------------------------\n",
      "Wonderland - GPE\n",
      "Lewis Carroll - PERSON\n",
      "Wonderland - GPE\n",
      "Cheshire Cat - ORGANIZATION\n",
      "Mad Hatter - ORGANIZATION\n",
      "Hearts - GPE\n",
      "Alice - ORGANIZATION\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and POS Tag\n",
    "words = word_tokenize(text)\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Extract named entities\n",
    "named_entities_tree = ne_chunk(pos_tags)\n",
    "\n",
    "print(\"Named Entities using nltk:\")\n",
    "print(\"-\" * 50)\n",
    "for subtree in named_entities_tree.subtrees(filter=lambda t: t.label() in ['GPE', 'PERSON', 'ORGANIZATION', 'DATE']):\n",
    "    entity_name = ' '.join([word for word, tag in subtree.leaves()])\n",
    "    print(entity_name, \"-\", subtree.label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d109da8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common NER Tags in nltk:\n",
      "--------------------------------------------------\n",
      "GPE                 : Geopolitical Entity\n",
      "PERSON              : Person\n",
      "ORGANIZATION        : Organization\n",
      "LOCATION            : Location\n",
      "DATE                : Date\n",
      "TIME                : Time\n",
      "MONEY               : Money\n",
      "PERCENT             : Percentage\n",
      "FACILITY            : Facility\n",
      "GSP                 : Geopolitical Subdivision\n"
     ]
    }
   ],
   "source": [
    "print(\"Common NER Tags in nltk:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "nltk_tags = {\n",
    "    \"GPE\": \"Geopolitical Entity\",\n",
    "    \"PERSON\": \"Person\",\n",
    "    \"ORGANIZATION\": \"Organization\",\n",
    "    \"LOCATION\": \"Location\",\n",
    "    \"DATE\": \"Date\",\n",
    "    \"TIME\": \"Time\",\n",
    "    \"MONEY\": \"Money\",\n",
    "    \"PERCENT\": \"Percentage\",\n",
    "    \"FACILITY\": \"Facility\",\n",
    "    \"GSP\": \"Geopolitical Subdivision\"\n",
    "}\n",
    "\n",
    "for tag, description in nltk_tags.items():\n",
    "    print(f\"{tag.ljust(20)}: {description}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "95d34d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Alice in Wonderland,\" penned by Lewis Carroll, narrates the whimsical tale of a young girl named Alice who tumbles down a rabbit hole and finds herself in a fantastical world filled with bizarre creatures and perplexing situations. As she navigates this Wonderland, she encounters a myriad of eccentric characters, including the Cheshire Cat, the Mad Hatter, the Queen of Hearts, and the White Rabbit, each of whom adds to her bewildering adventure. Throughout her journey, Alice faces challenges that test her logic and reason, ultimately leading her to question the very nature of reality before she finally awakens, realizing her adventures were but a dream.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20189be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "26f68758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'White', 'Rabbit', 'Wonderland']\n"
     ]
    }
   ],
   "source": [
    "# Extracting ALL the noun tags only\n",
    "def extract_nouns(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # POS tagging\n",
    "    tagged = pos_tag(words)\n",
    "    \n",
    "    # Filter noun tags\n",
    "    noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    nouns = [word for word, tag in tagged if tag in noun_tags]\n",
    "    \n",
    "    return nouns\n",
    "\n",
    "text = \"Alice and the White Rabbit entered Wonderland.\"\n",
    "print(extract_nouns(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2bcb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711364f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da30a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581f7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9c60887f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities using spaCy:\n",
      "--------------------------------------------------\n",
      "PM - ORG\n",
      "G20 - MISC\n",
      "Brazil's Lula - PER\n",
      "G20 - MISC\n",
      "Delhi - LOC\n",
      "The Prime - MISC\n",
      "Brazilian President to mark the transfer of the Presidency at the - MISC\n",
      "G20 - MISC\n",
      "Mandapam - LOC\n",
      "Delhi's - LOC\n",
      "Pragati Maidan - PER\n",
      "With - PER\n",
      "G20 Presidency in the national capital - MISC\n"
     ]
    }
   ],
   "source": [
    "# Spacy\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Named Entities using spaCy:\")\n",
    "print(\"-\" * 50)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"-\", ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4f9ffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Tags in spaCy:\n",
      "--------------------------------------------------\n",
      "CARDINAL            : Numerals that do not fall under another type\n",
      "DATE                : Absolute or relative dates or periods\n",
      "EVENT               : Named hurricanes, battles, wars, sports events, etc.\n",
      "FAC                 : Buildings, airports, highways, bridges, etc.\n",
      "GPE                 : Countries, cities, states\n",
      "LANGUAGE            : Any named language\n",
      "LAW                 : Named documents made into laws.\n",
      "LOC                 : Non-GPE locations, mountain ranges, bodies of water\n",
      "MONEY               : Monetary values, including unit\n",
      "NORP                : Nationalities or religious or political groups\n",
      "ORDINAL             : \"first\", \"second\", etc.\n",
      "ORG                 : Companies, agencies, institutions, etc.\n",
      "PERCENT             : Percentage, including \"%\"\n",
      "PERSON              : People, including fictional\n",
      "PRODUCT             : Objects, vehicles, foods, etc. (not services)\n",
      "QUANTITY            : Measurements, as of weight or distance\n",
      "TIME                : Times smaller than a day\n",
      "WORK_OF_ART         : Titles of books, songs, etc.\n"
     ]
    }
   ],
   "source": [
    "# Access the NER pipeline component\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "print(\"NER Tags in spaCy:\")\n",
    "print(\"-\" * 50)\n",
    "for label in ner.labels:\n",
    "    print(f\"{label.ljust(20)}: {spacy.explain(label)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaac3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f634a974",
   "metadata": {},
   "source": [
    "# Syntax parsing\n",
    "\n",
    "Syntax parsing refers to the process of analyzing a text, which corresponds to the syntactic structure of the sentence according to a given grammar. The result of this analysis is often represented in a parse tree.\n",
    "\n",
    "nltk provides a context-free grammar parser. For this, you would first need to define a grammar (or load a predefined one) and then use it to parse a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a16cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import CFG\n",
    "from nltk.parse.chart import ChartParser\n",
    "\n",
    "# Define a simple grammar\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    PP -> P NP\n",
    "    NP -> Det N | Det N PP | 'I'\n",
    "    VP -> V NP | VP PP\n",
    "    Det -> 'an' | 'my'\n",
    "    N -> 'elephant' | 'pajamas'\n",
    "    V -> 'shot'\n",
    "    P -> 'in'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722adb4",
   "metadata": {},
   "source": [
    "This section creates a simple context-free grammar using the CFG.fromstring() method. Each rule defines how certain linguistic structures can be formed:\n",
    "\n",
    "- S -> NP VP: A sentence (S) is made up of a noun phrase (NP) followed by a verb phrase (VP).\n",
    "- PP -> P NP: A prepositional phrase (PP) is made of a preposition (P) followed by a noun phrase (NP).\n",
    "- NP -> Det N | Det N PP | 'I': A noun phrase can be a determiner (Det) followed by a noun (N), a determiner followed by a noun and then a prepositional phrase, or simply the word 'I'.\n",
    "- VP -> V NP | VP PP: A verb phrase can be a verb (V) followed by a noun phrase, or a verb phrase followed by a prepositional phrase.\n",
    "\n",
    "The subsequent rules define the valid words for each category:\n",
    "- Det -> 'an' | 'my': Valid determiners are \"an\" and \"my\".\n",
    "- N -> 'elephant' | 'pajamas': Valid nouns are \"elephant\" and \"pajamas\".\n",
    "- V -> 'shot': The only verb in this grammar is \"shot\".\n",
    "- P -> 'in': The only preposition is \"in\".\n",
    "\n",
    "The defined grammar is meant to capture simple sentences and is motivated by the classic syntactic ambiguity example: \"I shot an elephant in my pajamas.\" This sentence can be interpreted in two ways based on the grammar:\n",
    "\n",
    "I shot an elephant while I was in my pajamas.\n",
    "\n",
    "I shot an elephant that was in my pajamas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e089b86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     S                                       \n",
      "  ___|______________                          \n",
      " |                  VP                       \n",
      " |         _________|__________               \n",
      " |        VP                   PP            \n",
      " |    ____|___              ___|___           \n",
      " |   |        NP           |       NP        \n",
      " |   |     ___|_____       |    ___|_____     \n",
      " NP  V   Det        N      P  Det        N   \n",
      " |   |    |         |      |   |         |    \n",
      " I  shot  an     elephant  in  my     pajamas\n",
      "\n",
      "     S                                   \n",
      "  ___|__________                          \n",
      " |              VP                       \n",
      " |    __________|______                   \n",
      " |   |                 NP                \n",
      " |   |     ____________|___               \n",
      " |   |    |     |          PP            \n",
      " |   |    |     |       ___|___           \n",
      " |   |    |     |      |       NP        \n",
      " |   |    |     |      |    ___|_____     \n",
      " NP  V   Det    N      P  Det        N   \n",
      " |   |    |     |      |   |         |    \n",
      " I  shot  an elephant  in  my     pajamas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parser = ChartParser(grammar)\n",
    "\n",
    "sentence = word_tokenize(\"I shot an elephant in my pajamas\")\n",
    "for tree in parser.parse(sentence):\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b88a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e072831f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7ee718848be14cd48cdd69c598fef276-0\" class=\"displacy\" width=\"1100\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">cat</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">sat</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">mat.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ee718848be14cd48cdd69c598fef276-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ee718848be14cd48cdd69c598fef276-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ee718848be14cd48cdd69c598fef276-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ee718848be14cd48cdd69c598fef276-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ee718848be14cd48cdd69c598fef276-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ee718848be14cd48cdd69c598fef276-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ee718848be14cd48cdd69c598fef276-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ee718848be14cd48cdd69c598fef276-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,179.0 L762,167.0 778,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ee718848be14cd48cdd69c598fef276-0-4\" stroke-width=\"2px\" d=\"M595,177.0 C595,2.0 925.0,2.0 925.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ee718848be14cd48cdd69c598fef276-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,179.0 L933.0,167.0 917.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "sentence = \"The cat sat on the mat.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Using displacy.render() to visualize the dependency tree\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)  # Use jupyter=False if not in a Jupyter environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af049e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3e7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868673b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a14acfde",
   "metadata": {},
   "source": [
    "# TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d19cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "17c0e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "news1 = \"\"\"PM Modi, Macron hold lunch meet, eye India-France ties at new heights of progress\n",
    "French president Emmanuel Macron said India did its utmost for the G-20 presidency to serve unity and peace.\n",
    "Prime Minister Narendra Modi on Sunday had lunch with French president Emmanuel Macron following the conclusion of the G20 Summit.\n",
    "\n",
    "Prime Minister Narendra Modi with French president Emmanuel Macron.(X/Narendra Modi)\n",
    "“A very productive lunch meeting with President @EmmanuelMacron. We discussed a series of topics and look forward to ensuring India-France relations scale new heights of progress”, PM Modi posted on social media platform X, formerly Twitter.\n",
    "\n",
    "The prime minister was accompanied by external affairs minister S Jaishankar and National Security Advisor Ajit Doval.\n",
    "\n",
    "“We will further develop defence cooperation with India”, Macron was quoted by PTI as saying.\n",
    "\n",
    "Earlier, Macron thanked Prime Minister Modi and hailed India's efforts for G20 presidency to serve peace and unity. “I thank PM Modi. Faithful to its principles India did its utmost for the G-20 presidency to serve unity and peace and send across the message of unity while Russia is still waging its aggression on Ukraine”, ANI quoted the French president as saying.\n",
    "\n",
    "Macron also called for deep reform of international organisations. “We support a deep reform of global governance. Security Council but as well the World Bank and the IMF, they have to reflect today's reality in terms of demography and economy as well. And then we want to increase the available tools. That's why we want to replenish the World Bank and France is supporting that so that the emerging countries have a greater role to play”, the French president said.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d3cbdb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "news2 = \"\"\"Shaheen dismissed Rohit, Kohli, so focus was on him. But...': Gavaskar alerts Team India against another Pakistan star\n",
    "\n",
    "The India batting great spoke in detail about the previous match between the two sides in the group stage.\n",
    "Team India is preparing for a highly anticipated showdown with Pakistan in the Asia Cup Super 4 stage on Sunday. The two sides previously crossed paths during the group stage, but the match was abandoned due to rain in Pallekele. India played out their whole innings, scoring 266, but the match did leave the Indian team management wit significant concerns over their top-order failure. India were left reeling at 66/4 with Rohit Sharma and Virat Kohli being castled by left-armer Shaheen Afridi, while fellow pacers Naseem Shah and Haris Rauf also making life miserable for the Indian batters.\n",
    "\n",
    "India's Virat Kohli walks back to the pavilion after his dismissal during the Asia Cup 2023 match between India and Pakistan(AFP)\n",
    "\n",
    "However, Hardik Pandya's resilient innings of 87 runs and Ishan Kishan's impressive 82 helped India recover from the early setbacks, guiding them to a total of 266. Among Pakistan's bowlers, star pacer Shaheen Afridi delivered a standout performance, registering impressive figures of 4/35. But while Afridi did end as the innings' leading wicket-taker, former India captain and batting great Sunil Gavaskar emphasised on the Naseem Shah threat as the side prepares to face Pakistan again.\n",
    "\n",
    "Also read: 'India have great players but I'm sorry, they're praising us too much': PAK great's hard-hitting remark on Rohit, Kohli\n",
    "Gavaskar pointed out that the right-armer was terrific in his opening spell and created problems for Shubman Gill, insisting that his lethal out-swingers made it difficult for the Indian batters in the start.\n",
    "\n",
    "“If you looked at those 10 overs, you must have noticed the way Naseem Shah bowled. His out-swingers were brilliant, playing him was pretty difficult. Shaheen Afridi did take two wickets of Rohit Sharma and Virat Kohli, so focus was on him. But the way Naseem bowled, that was terrific. Shubman Gill was playing majorly against him, and he was leaving him well,” Gavaskar said in a joint broadcast from Sports Tak and Samaa TV.\n",
    "\n",
    "“If batters keep getting out at other end, it's important to stick around. Shubman isn't in the best form right now, so he knows he has to take responsibility. This was a 50-over game after all. He has the shots to make up for slow start. I guess that's why he was watchful, and a bit nervous. But Naseem Shah kept Shubman Gill quiet,” said Gavaskar further.\n",
    "\n",
    "Rain threat looms large\n",
    "The upcoming India vs Pakistan match on Sunday faces a significant risk of being interrupted by rain again. The Asian Cricket Council triggered controversy earlier this week by designating a reserve day for the Super 4 encounter between the two arch-rivals, particularly because no other game in the Super 4 was allocated reserve day benefits.\n",
    "\n",
    "This decision was prompted by the high likelihood of rain on the scheduled match day, Sunday. However, the weather forecast indicates that there are also considerable chances of rain on Monday, which means that even the reserve day might not guarantee uninterrupted play.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1a466772",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news1 + news2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57fc8bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture \n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "53ce3e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.2.0'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63792dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download stopwords from NLTK\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7574a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lowercasing\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    # Removing punctuations\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f9b9b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = news\n",
    "tokens = preprocess(text)\n",
    "\n",
    "# Create a dictionary representation of the tokenized text\n",
    "dictionary = corpora.Dictionary([tokens])  # dictionary is SAME as \"VOCAB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "00eda59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328]\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e93fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a1b06f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 10),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 2),\n",
       "  (8, 3),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 4),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 4),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 2),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 2),\n",
       "  (37, 3),\n",
       "  (38, 2),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 2),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 1),\n",
       "  (51, 1),\n",
       "  (52, 1),\n",
       "  (53, 1),\n",
       "  (54, 1),\n",
       "  (55, 2),\n",
       "  (56, 1),\n",
       "  (57, 1),\n",
       "  (58, 1),\n",
       "  (59, 1),\n",
       "  (60, 2),\n",
       "  (61, 4),\n",
       "  (62, 1),\n",
       "  (63, 2),\n",
       "  (64, 1),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1),\n",
       "  (69, 1),\n",
       "  (70, 2),\n",
       "  (71, 1),\n",
       "  (72, 1),\n",
       "  (73, 1),\n",
       "  (74, 1),\n",
       "  (75, 1),\n",
       "  (76, 2),\n",
       "  (77, 1),\n",
       "  (78, 1),\n",
       "  (79, 1),\n",
       "  (80, 1),\n",
       "  (81, 3),\n",
       "  (82, 1),\n",
       "  (83, 1),\n",
       "  (84, 1),\n",
       "  (85, 2),\n",
       "  (86, 1),\n",
       "  (87, 1),\n",
       "  (88, 1),\n",
       "  (89, 1),\n",
       "  (90, 2),\n",
       "  (91, 1),\n",
       "  (92, 1),\n",
       "  (93, 1),\n",
       "  (94, 1),\n",
       "  (95, 2),\n",
       "  (96, 1),\n",
       "  (97, 1),\n",
       "  (98, 1),\n",
       "  (99, 1),\n",
       "  (100, 1),\n",
       "  (101, 1),\n",
       "  (102, 1),\n",
       "  (103, 5),\n",
       "  (104, 2),\n",
       "  (105, 2),\n",
       "  (106, 2),\n",
       "  (107, 5),\n",
       "  (108, 1),\n",
       "  (109, 3),\n",
       "  (110, 1),\n",
       "  (111, 1),\n",
       "  (112, 4),\n",
       "  (113, 1),\n",
       "  (114, 2),\n",
       "  (115, 1),\n",
       "  (116, 1),\n",
       "  (117, 1),\n",
       "  (118, 1),\n",
       "  (119, 1),\n",
       "  (120, 1),\n",
       "  (121, 1),\n",
       "  (122, 2),\n",
       "  (123, 1),\n",
       "  (124, 1),\n",
       "  (125, 1),\n",
       "  (126, 1),\n",
       "  (127, 2),\n",
       "  (128, 1),\n",
       "  (129, 1),\n",
       "  (130, 2),\n",
       "  (131, 1),\n",
       "  (132, 14),\n",
       "  (133, 2),\n",
       "  (134, 3),\n",
       "  (135, 1),\n",
       "  (136, 3),\n",
       "  (137, 1),\n",
       "  (138, 1),\n",
       "  (139, 1),\n",
       "  (140, 1),\n",
       "  (141, 1),\n",
       "  (142, 1),\n",
       "  (143, 1),\n",
       "  (144, 1),\n",
       "  (145, 1),\n",
       "  (146, 1),\n",
       "  (147, 5),\n",
       "  (148, 1),\n",
       "  (149, 1),\n",
       "  (150, 1),\n",
       "  (151, 1),\n",
       "  (152, 1),\n",
       "  (153, 1),\n",
       "  (154, 1),\n",
       "  (155, 1),\n",
       "  (156, 1),\n",
       "  (157, 1),\n",
       "  (158, 1),\n",
       "  (159, 1),\n",
       "  (160, 3),\n",
       "  (161, 7),\n",
       "  (162, 1),\n",
       "  (163, 1),\n",
       "  (164, 1),\n",
       "  (165, 1),\n",
       "  (166, 1),\n",
       "  (167, 6),\n",
       "  (168, 1),\n",
       "  (169, 1),\n",
       "  (170, 1),\n",
       "  (171, 1),\n",
       "  (172, 1),\n",
       "  (173, 1),\n",
       "  (174, 5),\n",
       "  (175, 1),\n",
       "  (176, 7),\n",
       "  (177, 1),\n",
       "  (178, 1),\n",
       "  (179, 1),\n",
       "  (180, 1),\n",
       "  (181, 2),\n",
       "  (182, 5),\n",
       "  (183, 1),\n",
       "  (184, 1),\n",
       "  (185, 2),\n",
       "  (186, 1),\n",
       "  (187, 1),\n",
       "  (188, 1),\n",
       "  (189, 2),\n",
       "  (190, 1),\n",
       "  (191, 2),\n",
       "  (192, 1),\n",
       "  (193, 6),\n",
       "  (194, 1),\n",
       "  (195, 1),\n",
       "  (196, 1),\n",
       "  (197, 1),\n",
       "  (198, 1),\n",
       "  (199, 3),\n",
       "  (200, 1),\n",
       "  (201, 1),\n",
       "  (202, 2),\n",
       "  (203, 1),\n",
       "  (204, 1),\n",
       "  (205, 2),\n",
       "  (206, 3),\n",
       "  (207, 1),\n",
       "  (208, 1),\n",
       "  (209, 1),\n",
       "  (210, 1),\n",
       "  (211, 1),\n",
       "  (212, 3),\n",
       "  (213, 6),\n",
       "  (214, 1),\n",
       "  (215, 1),\n",
       "  (216, 1),\n",
       "  (217, 4),\n",
       "  (218, 1),\n",
       "  (219, 1),\n",
       "  (220, 1),\n",
       "  (221, 2),\n",
       "  (222, 1),\n",
       "  (223, 1),\n",
       "  (224, 1),\n",
       "  (225, 2),\n",
       "  (226, 5),\n",
       "  (227, 1),\n",
       "  (228, 1),\n",
       "  (229, 1),\n",
       "  (230, 1),\n",
       "  (231, 1),\n",
       "  (232, 1),\n",
       "  (233, 2),\n",
       "  (234, 1),\n",
       "  (235, 1),\n",
       "  (236, 1),\n",
       "  (237, 1),\n",
       "  (238, 3),\n",
       "  (239, 1),\n",
       "  (240, 1),\n",
       "  (241, 1),\n",
       "  (242, 1),\n",
       "  (243, 1),\n",
       "  (244, 4),\n",
       "  (245, 1),\n",
       "  (246, 1),\n",
       "  (247, 1),\n",
       "  (248, 3),\n",
       "  (249, 1),\n",
       "  (250, 1),\n",
       "  (251, 2),\n",
       "  (252, 1),\n",
       "  (253, 1),\n",
       "  (254, 1),\n",
       "  (255, 2),\n",
       "  (256, 1),\n",
       "  (257, 1),\n",
       "  (258, 3),\n",
       "  (259, 1),\n",
       "  (260, 4),\n",
       "  (261, 3),\n",
       "  (262, 2),\n",
       "  (263, 1),\n",
       "  (264, 1),\n",
       "  (265, 4),\n",
       "  (266, 3),\n",
       "  (267, 2),\n",
       "  (268, 1),\n",
       "  (269, 1),\n",
       "  (270, 1),\n",
       "  (271, 1),\n",
       "  (272, 1),\n",
       "  (273, 1),\n",
       "  (274, 3),\n",
       "  (275, 1),\n",
       "  (276, 2),\n",
       "  (277, 2),\n",
       "  (278, 1),\n",
       "  (279, 1),\n",
       "  (280, 1),\n",
       "  (281, 4),\n",
       "  (282, 1),\n",
       "  (283, 3),\n",
       "  (284, 1),\n",
       "  (285, 1),\n",
       "  (286, 1),\n",
       "  (287, 2),\n",
       "  (288, 3),\n",
       "  (289, 1),\n",
       "  (290, 2),\n",
       "  (291, 1),\n",
       "  (292, 1),\n",
       "  (293, 2),\n",
       "  (294, 1),\n",
       "  (295, 1),\n",
       "  (296, 1),\n",
       "  (297, 1),\n",
       "  (298, 1),\n",
       "  (299, 1),\n",
       "  (300, 1),\n",
       "  (301, 1),\n",
       "  (302, 1),\n",
       "  (303, 4),\n",
       "  (304, 1),\n",
       "  (305, 1),\n",
       "  (306, 1),\n",
       "  (307, 4),\n",
       "  (308, 1),\n",
       "  (309, 2),\n",
       "  (310, 1),\n",
       "  (311, 3),\n",
       "  (312, 1),\n",
       "  (313, 1),\n",
       "  (314, 2),\n",
       "  (315, 1),\n",
       "  (316, 2),\n",
       "  (317, 1),\n",
       "  (318, 1),\n",
       "  (319, 3),\n",
       "  (320, 1),\n",
       "  (321, 1),\n",
       "  (322, 1),\n",
       "  (323, 1),\n",
       "  (324, 2),\n",
       "  (325, 1),\n",
       "  (326, 1),\n",
       "  (327, 6),\n",
       "  (328, 6)]]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the list of tokens into a bag of words corpus\n",
    "bow_corpus = [dictionary.doc2bow(tokens)]\n",
    "bow_corpus  # list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d9809274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "51746e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 10),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 2),\n",
       " (8, 3),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 4),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 4),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 2),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 2),\n",
       " (37, 3),\n",
       " (38, 2),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 2),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 1),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 2),\n",
       " (56, 1),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 2),\n",
       " (61, 4),\n",
       " (62, 1),\n",
       " (63, 2),\n",
       " (64, 1),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 2),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 1),\n",
       " (74, 1),\n",
       " (75, 1),\n",
       " (76, 2),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 3),\n",
       " (82, 1),\n",
       " (83, 1),\n",
       " (84, 1),\n",
       " (85, 2),\n",
       " (86, 1),\n",
       " (87, 1),\n",
       " (88, 1),\n",
       " (89, 1),\n",
       " (90, 2),\n",
       " (91, 1),\n",
       " (92, 1),\n",
       " (93, 1),\n",
       " (94, 1),\n",
       " (95, 2),\n",
       " (96, 1),\n",
       " (97, 1),\n",
       " (98, 1),\n",
       " (99, 1),\n",
       " (100, 1),\n",
       " (101, 1),\n",
       " (102, 1),\n",
       " (103, 5),\n",
       " (104, 2),\n",
       " (105, 2),\n",
       " (106, 2),\n",
       " (107, 5),\n",
       " (108, 1),\n",
       " (109, 3),\n",
       " (110, 1),\n",
       " (111, 1),\n",
       " (112, 4),\n",
       " (113, 1),\n",
       " (114, 2),\n",
       " (115, 1),\n",
       " (116, 1),\n",
       " (117, 1),\n",
       " (118, 1),\n",
       " (119, 1),\n",
       " (120, 1),\n",
       " (121, 1),\n",
       " (122, 2),\n",
       " (123, 1),\n",
       " (124, 1),\n",
       " (125, 1),\n",
       " (126, 1),\n",
       " (127, 2),\n",
       " (128, 1),\n",
       " (129, 1),\n",
       " (130, 2),\n",
       " (131, 1),\n",
       " (132, 14),\n",
       " (133, 2),\n",
       " (134, 3),\n",
       " (135, 1),\n",
       " (136, 3),\n",
       " (137, 1),\n",
       " (138, 1),\n",
       " (139, 1),\n",
       " (140, 1),\n",
       " (141, 1),\n",
       " (142, 1),\n",
       " (143, 1),\n",
       " (144, 1),\n",
       " (145, 1),\n",
       " (146, 1),\n",
       " (147, 5),\n",
       " (148, 1),\n",
       " (149, 1),\n",
       " (150, 1),\n",
       " (151, 1),\n",
       " (152, 1),\n",
       " (153, 1),\n",
       " (154, 1),\n",
       " (155, 1),\n",
       " (156, 1),\n",
       " (157, 1),\n",
       " (158, 1),\n",
       " (159, 1),\n",
       " (160, 3),\n",
       " (161, 7),\n",
       " (162, 1),\n",
       " (163, 1),\n",
       " (164, 1),\n",
       " (165, 1),\n",
       " (166, 1),\n",
       " (167, 6),\n",
       " (168, 1),\n",
       " (169, 1),\n",
       " (170, 1),\n",
       " (171, 1),\n",
       " (172, 1),\n",
       " (173, 1),\n",
       " (174, 5),\n",
       " (175, 1),\n",
       " (176, 7),\n",
       " (177, 1),\n",
       " (178, 1),\n",
       " (179, 1),\n",
       " (180, 1),\n",
       " (181, 2),\n",
       " (182, 5),\n",
       " (183, 1),\n",
       " (184, 1),\n",
       " (185, 2),\n",
       " (186, 1),\n",
       " (187, 1),\n",
       " (188, 1),\n",
       " (189, 2),\n",
       " (190, 1),\n",
       " (191, 2),\n",
       " (192, 1),\n",
       " (193, 6),\n",
       " (194, 1),\n",
       " (195, 1),\n",
       " (196, 1),\n",
       " (197, 1),\n",
       " (198, 1),\n",
       " (199, 3),\n",
       " (200, 1),\n",
       " (201, 1),\n",
       " (202, 2),\n",
       " (203, 1),\n",
       " (204, 1),\n",
       " (205, 2),\n",
       " (206, 3),\n",
       " (207, 1),\n",
       " (208, 1),\n",
       " (209, 1),\n",
       " (210, 1),\n",
       " (211, 1),\n",
       " (212, 3),\n",
       " (213, 6),\n",
       " (214, 1),\n",
       " (215, 1),\n",
       " (216, 1),\n",
       " (217, 4),\n",
       " (218, 1),\n",
       " (219, 1),\n",
       " (220, 1),\n",
       " (221, 2),\n",
       " (222, 1),\n",
       " (223, 1),\n",
       " (224, 1),\n",
       " (225, 2),\n",
       " (226, 5),\n",
       " (227, 1),\n",
       " (228, 1),\n",
       " (229, 1),\n",
       " (230, 1),\n",
       " (231, 1),\n",
       " (232, 1),\n",
       " (233, 2),\n",
       " (234, 1),\n",
       " (235, 1),\n",
       " (236, 1),\n",
       " (237, 1),\n",
       " (238, 3),\n",
       " (239, 1),\n",
       " (240, 1),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 1),\n",
       " (244, 4),\n",
       " (245, 1),\n",
       " (246, 1),\n",
       " (247, 1),\n",
       " (248, 3),\n",
       " (249, 1),\n",
       " (250, 1),\n",
       " (251, 2),\n",
       " (252, 1),\n",
       " (253, 1),\n",
       " (254, 1),\n",
       " (255, 2),\n",
       " (256, 1),\n",
       " (257, 1),\n",
       " (258, 3),\n",
       " (259, 1),\n",
       " (260, 4),\n",
       " (261, 3),\n",
       " (262, 2),\n",
       " (263, 1),\n",
       " (264, 1),\n",
       " (265, 4),\n",
       " (266, 3),\n",
       " (267, 2),\n",
       " (268, 1),\n",
       " (269, 1),\n",
       " (270, 1),\n",
       " (271, 1),\n",
       " (272, 1),\n",
       " (273, 1),\n",
       " (274, 3),\n",
       " (275, 1),\n",
       " (276, 2),\n",
       " (277, 2),\n",
       " (278, 1),\n",
       " (279, 1),\n",
       " (280, 1),\n",
       " (281, 4),\n",
       " (282, 1),\n",
       " (283, 3),\n",
       " (284, 1),\n",
       " (285, 1),\n",
       " (286, 1),\n",
       " (287, 2),\n",
       " (288, 3),\n",
       " (289, 1),\n",
       " (290, 2),\n",
       " (291, 1),\n",
       " (292, 1),\n",
       " (293, 2),\n",
       " (294, 1),\n",
       " (295, 1),\n",
       " (296, 1),\n",
       " (297, 1),\n",
       " (298, 1),\n",
       " (299, 1),\n",
       " (300, 1),\n",
       " (301, 1),\n",
       " (302, 1),\n",
       " (303, 4),\n",
       " (304, 1),\n",
       " (305, 1),\n",
       " (306, 1),\n",
       " (307, 4),\n",
       " (308, 1),\n",
       " (309, 2),\n",
       " (310, 1),\n",
       " (311, 3),\n",
       " (312, 1),\n",
       " (313, 1),\n",
       " (314, 2),\n",
       " (315, 1),\n",
       " (316, 2),\n",
       " (317, 1),\n",
       " (318, 1),\n",
       " (319, 3),\n",
       " (320, 1),\n",
       " (321, 1),\n",
       " (322, 1),\n",
       " (323, 1),\n",
       " (324, 2),\n",
       " (325, 1),\n",
       " (326, 1),\n",
       " (327, 6),\n",
       " (328, 6)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[0]  # extracting the first nested list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "429fcbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'formerly'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[100]  # text against the id 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8663bcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'india\", \"'m\", \"'re\", \"'s\", '...', '10', '2023', '266', '4', '4/35', '50-over', '66/4', '82', '87', 'abandoned', 'accompanied', 'across', 'advisor', 'affair', 'afp', 'afridi', 'aggression', 'ajit', 'alert', 'allocated', 'also', 'among', 'ani', 'another', 'anticipated', 'arch-rivals', 'around', 'asia', 'asian', 'available', 'back', 'bank', 'batter', 'batting', 'benefit', 'best', 'bit', 'bowled', 'bowler', 'brilliant', 'broadcast', 'called', 'captain', 'castled', 'chance', 'concern', 'conclusion', 'considerable', 'controversy', 'cooperation', 'council', 'country', 'created', 'cricket', 'crossed', 'cup', 'day', 'decision', 'deep', 'defence', 'delivered', 'demography', 'designating', 'detail', 'develop', 'difficult', 'discussed', 'dismissal', 'dismissed', 'doval', 'due', 'earlier', 'early', 'economy', 'effort', 'emerging', 'emmanuel', 'emmanuelmacron', 'emphasised', 'encounter', 'end', 'ensuring', 'even', 'external', 'eye', 'face', 'failure', 'faithful', 'fellow', 'figure', 'focus', 'following', 'forecast', 'form', 'former', 'formerly', 'forward', 'france', 'french', 'g-20', 'g20', 'game', 'gavaskar', 'getting', 'gill', 'global', 'governance', 'great', 'greater', 'group', 'guarantee', 'guess', 'guiding', 'hailed', 'hard-hitting', 'hardik', 'haris', 'height', 'helped', 'high', 'highly', 'hold', 'however', 'imf', 'important', 'impressive', 'increase', 'india', 'india-france', 'indian', 'indicates', 'inning', 'insisting', 'international', 'interrupted', 'ishan', 'jaishankar', 'joint', 'keep', 'kept', 'kishan', 'know', 'kohli', 'large', 'leading', 'leave', 'leaving', 'left', 'left-armer', 'lethal', 'life', 'likelihood', 'look', 'looked', 'loom', 'lunch', 'macron', 'made', 'majorly', 'make', 'making', 'management', 'match', 'mean', 'medium', 'meet', 'meeting', 'message', 'might', 'minister', 'miserable', 'modi', 'monday', 'much', 'must', \"n't\", 'narendra', 'naseem', 'national', 'nervous', 'new', 'noticed', 'opening', 'organisation', 'out-swingers', 'over', 'pacer', 'pak', 'pakistan', 'pallekele', 'pandya', 'particularly', 'path', 'pavilion', 'peace', 'performance', 'platform', 'play', 'played', 'player', 'playing', 'pm', 'pointed', 'posted', 'praising', 'prepares', 'preparing', 'presidency', 'president', 'pretty', 'previous', 'previously', 'prime', 'principle', 'problem', 'productive', 'progress', 'prompted', 'pti', 'quiet', 'quoted', 'rain', 'rauf', 'read', 'reality', 'recover', 'reeling', 'reflect', 'reform', 'registering', 'relation', 'remark', 'replenish', 'reserve', 'resilient', 'responsibility', 'right', 'right-armer', 'risk', 'rohit', 'role', 'run', 'russia', 'said', 'said.shaheen', 'samaa', 'saying', 'scale', 'scheduled', 'scoring', 'security', 'send', 'series', 'serve', 'setback', 'shah', 'shaheen', 'sharma', 'shot', 'showdown', 'shubman', 'side', 'significant', 'slow', 'social', 'sorry', 'spell', 'spoke', 'sport', 'stage', 'standout', 'star', 'start', 'stick', 'still', 'summit', 'sunday', 'sunil', 'super', 'support', 'supporting', 'tak', 'take', 'team', 'term', 'terrific', 'thank', 'thanked', 'threat', 'tie', 'today', 'tool', 'top-order', 'topic', 'total', 'triggered', 'tv', 'twitter', 'two', 'u', 'ukraine', 'uninterrupted', 'unity', 'upcoming', 'utmost', 'v', 'virat', 'waging', 'walk', 'want', 'watchful', 'way', 'weather', 'week', 'well', 'whole', 'wicket', 'wicket-taker', 'wit', 'world', 'x', 'x/narendra', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "# Extracting tokenized text from dictionary\n",
    "extracted_tokens = [dictionary[id1] for id1, freq in bow_corpus[0]]\n",
    "\n",
    "print(extracted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f2a0b33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PM Modi, Macron hold lunch meet, eye India-France ties at new heights of progress\\nFrench president Emmanuel Macron said India did its utmost for the G-20 presidency to serve unity and peace.\\nPrime Minister Narendra Modi on Sunday had lunch with French president Emmanuel Macron following the conclusion of the G20 Summit.\\n\\nPrime Minister Narendra Modi with French president Emmanuel Macron.(X/Narendra Modi)\\n“A very productive lunch meeting with President @EmmanuelMacron. We discussed a series of topics and look forward to ensuring India-France relations scale new heights of progress”, PM Modi posted on social media platform X, formerly Twitter.\\n\\nThe prime minister was accompanied by external affairs minister S Jaishankar and National Security Advisor Ajit Doval.\\n\\n“We will further develop defence cooperation with India”, Macron was quoted by PTI as saying.\\n\\nEarlier, Macron thanked Prime Minister Modi and hailed India's efforts for G20 presidency to serve peace and unity. “I thank PM Modi. Faithful to its principles India did its utmost for the G-20 presidency to serve unity and peace and send across the message of unity while Russia is still waging its aggression on Ukraine”, ANI quoted the French president as saying.\\n\\nMacron also called for deep reform of international organisations. “We support a deep reform of global governance. Security Council but as well the World Bank and the IMF, they have to reflect today's reality in terms of demography and economy as well. And then we want to increase the available tools. That's why we want to replenish the World Bank and France is supporting that so that the emerging countries have a greater role to play”, the French president said.Shaheen dismissed Rohit, Kohli, so focus was on him. But...': Gavaskar alerts Team India against another Pakistan star\\n\\nThe India batting great spoke in detail about the previous match between the two sides in the group stage.\\nTeam India is preparing for a highly anticipated showdown with Pakistan in the Asia Cup Super 4 stage on Sunday. The two sides previously crossed paths during the group stage, but the match was abandoned due to rain in Pallekele. India played out their whole innings, scoring 266, but the match did leave the Indian team management wit significant concerns over their top-order failure. India were left reeling at 66/4 with Rohit Sharma and Virat Kohli being castled by left-armer Shaheen Afridi, while fellow pacers Naseem Shah and Haris Rauf also making life miserable for the Indian batters.\\n\\nIndia's Virat Kohli walks back to the pavilion after his dismissal during the Asia Cup 2023 match between India and Pakistan(AFP)\\n\\nHowever, Hardik Pandya's resilient innings of 87 runs and Ishan Kishan's impressive 82 helped India recover from the early setbacks, guiding them to a total of 266. Among Pakistan's bowlers, star pacer Shaheen Afridi delivered a standout performance, registering impressive figures of 4/35. But while Afridi did end as the innings' leading wicket-taker, former India captain and batting great Sunil Gavaskar emphasised on the Naseem Shah threat as the side prepares to face Pakistan again.\\n\\nAlso read: 'India have great players but I'm sorry, they're praising us too much': PAK great's hard-hitting remark on Rohit, Kohli\\nGavaskar pointed out that the right-armer was terrific in his opening spell and created problems for Shubman Gill, insisting that his lethal out-swingers made it difficult for the Indian batters in the start.\\n\\n“If you looked at those 10 overs, you must have noticed the way Naseem Shah bowled. His out-swingers were brilliant, playing him was pretty difficult. Shaheen Afridi did take two wickets of Rohit Sharma and Virat Kohli, so focus was on him. But the way Naseem bowled, that was terrific. Shubman Gill was playing majorly against him, and he was leaving him well,” Gavaskar said in a joint broadcast from Sports Tak and Samaa TV.\\n\\n“If batters keep getting out at other end, it's important to stick around. Shubman isn't in the best form right now, so he knows he has to take responsibility. This was a 50-over game after all. He has the shots to make up for slow start. I guess that's why he was watchful, and a bit nervous. But Naseem Shah kept Shubman Gill quiet,” said Gavaskar further.\\n\\nRain threat looms large\\nThe upcoming India vs Pakistan match on Sunday faces a significant risk of being interrupted by rain again. The Asian Cricket Council triggered controversy earlier this week by designating a reserve day for the Super 4 encounter between the two arch-rivals, particularly because no other game in the Super 4 was allocated reserve day benefits.\\n\\nThis decision was prompted by the high likelihood of rain on the scheduled match day, Sunday. However, the weather forecast indicates that there are also considerable chances of rain on Monday, which means that even the reserve day might not guarantee uninterrupted play.\""
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1200c793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pm',\n",
       " 'modi',\n",
       " 'macron',\n",
       " 'hold',\n",
       " 'lunch',\n",
       " 'meet',\n",
       " 'eye',\n",
       " 'india-france',\n",
       " 'tie',\n",
       " 'new',\n",
       " 'height',\n",
       " 'progress',\n",
       " 'french',\n",
       " 'president',\n",
       " 'emmanuel',\n",
       " 'macron',\n",
       " 'said',\n",
       " 'india',\n",
       " 'utmost',\n",
       " 'g-20',\n",
       " 'presidency',\n",
       " 'serve',\n",
       " 'unity',\n",
       " 'peace',\n",
       " 'prime',\n",
       " 'minister',\n",
       " 'narendra',\n",
       " 'modi',\n",
       " 'sunday',\n",
       " 'lunch',\n",
       " 'french',\n",
       " 'president',\n",
       " 'emmanuel',\n",
       " 'macron',\n",
       " 'following',\n",
       " 'conclusion',\n",
       " 'g20',\n",
       " 'summit',\n",
       " 'prime',\n",
       " 'minister',\n",
       " 'narendra',\n",
       " 'modi',\n",
       " 'french',\n",
       " 'president',\n",
       " 'emmanuel',\n",
       " 'macron',\n",
       " 'x/narendra',\n",
       " 'modi',\n",
       " '“',\n",
       " 'productive',\n",
       " 'lunch',\n",
       " 'meeting',\n",
       " 'president',\n",
       " 'emmanuelmacron',\n",
       " 'discussed',\n",
       " 'series',\n",
       " 'topic',\n",
       " 'look',\n",
       " 'forward',\n",
       " 'ensuring',\n",
       " 'india-france',\n",
       " 'relation',\n",
       " 'scale',\n",
       " 'new',\n",
       " 'height',\n",
       " 'progress',\n",
       " '”',\n",
       " 'pm',\n",
       " 'modi',\n",
       " 'posted',\n",
       " 'social',\n",
       " 'medium',\n",
       " 'platform',\n",
       " 'x',\n",
       " 'formerly',\n",
       " 'twitter',\n",
       " 'prime',\n",
       " 'minister',\n",
       " 'accompanied',\n",
       " 'external',\n",
       " 'affair',\n",
       " 'minister',\n",
       " 'jaishankar',\n",
       " 'national',\n",
       " 'security',\n",
       " 'advisor',\n",
       " 'ajit',\n",
       " 'doval',\n",
       " '“',\n",
       " 'develop',\n",
       " 'defence',\n",
       " 'cooperation',\n",
       " 'india',\n",
       " '”',\n",
       " 'macron',\n",
       " 'quoted',\n",
       " 'pti',\n",
       " 'saying',\n",
       " 'earlier',\n",
       " 'macron',\n",
       " 'thanked',\n",
       " 'prime',\n",
       " 'minister',\n",
       " 'modi',\n",
       " 'hailed',\n",
       " 'india',\n",
       " \"'s\",\n",
       " 'effort',\n",
       " 'g20',\n",
       " 'presidency',\n",
       " 'serve',\n",
       " 'peace',\n",
       " 'unity',\n",
       " '“',\n",
       " 'thank',\n",
       " 'pm',\n",
       " 'modi',\n",
       " 'faithful',\n",
       " 'principle',\n",
       " 'india',\n",
       " 'utmost',\n",
       " 'g-20',\n",
       " 'presidency',\n",
       " 'serve',\n",
       " 'unity',\n",
       " 'peace',\n",
       " 'send',\n",
       " 'across',\n",
       " 'message',\n",
       " 'unity',\n",
       " 'russia',\n",
       " 'still',\n",
       " 'waging',\n",
       " 'aggression',\n",
       " 'ukraine',\n",
       " '”',\n",
       " 'ani',\n",
       " 'quoted',\n",
       " 'french',\n",
       " 'president',\n",
       " 'saying',\n",
       " 'macron',\n",
       " 'also',\n",
       " 'called',\n",
       " 'deep',\n",
       " 'reform',\n",
       " 'international',\n",
       " 'organisation',\n",
       " '“',\n",
       " 'support',\n",
       " 'deep',\n",
       " 'reform',\n",
       " 'global',\n",
       " 'governance',\n",
       " 'security',\n",
       " 'council',\n",
       " 'well',\n",
       " 'world',\n",
       " 'bank',\n",
       " 'imf',\n",
       " 'reflect',\n",
       " 'today',\n",
       " \"'s\",\n",
       " 'reality',\n",
       " 'term',\n",
       " 'demography',\n",
       " 'economy',\n",
       " 'well',\n",
       " 'want',\n",
       " 'increase',\n",
       " 'available',\n",
       " 'tool',\n",
       " \"'s\",\n",
       " 'want',\n",
       " 'replenish',\n",
       " 'world',\n",
       " 'bank',\n",
       " 'france',\n",
       " 'supporting',\n",
       " 'emerging',\n",
       " 'country',\n",
       " 'greater',\n",
       " 'role',\n",
       " 'play',\n",
       " '”',\n",
       " 'french',\n",
       " 'president',\n",
       " 'said.shaheen',\n",
       " 'dismissed',\n",
       " 'rohit',\n",
       " 'kohli',\n",
       " 'focus',\n",
       " '...',\n",
       " 'gavaskar',\n",
       " 'alert',\n",
       " 'team',\n",
       " 'india',\n",
       " 'another',\n",
       " 'pakistan',\n",
       " 'star',\n",
       " 'india',\n",
       " 'batting',\n",
       " 'great',\n",
       " 'spoke',\n",
       " 'detail',\n",
       " 'previous',\n",
       " 'match',\n",
       " 'two',\n",
       " 'side',\n",
       " 'group',\n",
       " 'stage',\n",
       " 'team',\n",
       " 'india',\n",
       " 'preparing',\n",
       " 'highly',\n",
       " 'anticipated',\n",
       " 'showdown',\n",
       " 'pakistan',\n",
       " 'asia',\n",
       " 'cup',\n",
       " 'super',\n",
       " '4',\n",
       " 'stage',\n",
       " 'sunday',\n",
       " 'two',\n",
       " 'side',\n",
       " 'previously',\n",
       " 'crossed',\n",
       " 'path',\n",
       " 'group',\n",
       " 'stage',\n",
       " 'match',\n",
       " 'abandoned',\n",
       " 'due',\n",
       " 'rain',\n",
       " 'pallekele',\n",
       " 'india',\n",
       " 'played',\n",
       " 'whole',\n",
       " 'inning',\n",
       " 'scoring',\n",
       " '266',\n",
       " 'match',\n",
       " 'leave',\n",
       " 'indian',\n",
       " 'team',\n",
       " 'management',\n",
       " 'wit',\n",
       " 'significant',\n",
       " 'concern',\n",
       " 'top-order',\n",
       " 'failure',\n",
       " 'india',\n",
       " 'left',\n",
       " 'reeling',\n",
       " '66/4',\n",
       " 'rohit',\n",
       " 'sharma',\n",
       " 'virat',\n",
       " 'kohli',\n",
       " 'castled',\n",
       " 'left-armer',\n",
       " 'shaheen',\n",
       " 'afridi',\n",
       " 'fellow',\n",
       " 'pacer',\n",
       " 'naseem',\n",
       " 'shah',\n",
       " 'haris',\n",
       " 'rauf',\n",
       " 'also',\n",
       " 'making',\n",
       " 'life',\n",
       " 'miserable',\n",
       " 'indian',\n",
       " 'batter',\n",
       " 'india',\n",
       " \"'s\",\n",
       " 'virat',\n",
       " 'kohli',\n",
       " 'walk',\n",
       " 'back',\n",
       " 'pavilion',\n",
       " 'dismissal',\n",
       " 'asia',\n",
       " 'cup',\n",
       " '2023',\n",
       " 'match',\n",
       " 'india',\n",
       " 'pakistan',\n",
       " 'afp',\n",
       " 'however',\n",
       " 'hardik',\n",
       " 'pandya',\n",
       " \"'s\",\n",
       " 'resilient',\n",
       " 'inning',\n",
       " '87',\n",
       " 'run',\n",
       " 'ishan',\n",
       " 'kishan',\n",
       " \"'s\",\n",
       " 'impressive',\n",
       " '82',\n",
       " 'helped',\n",
       " 'india',\n",
       " 'recover',\n",
       " 'early',\n",
       " 'setback',\n",
       " 'guiding',\n",
       " 'total',\n",
       " '266',\n",
       " 'among',\n",
       " 'pakistan',\n",
       " \"'s\",\n",
       " 'bowler',\n",
       " 'star',\n",
       " 'pacer',\n",
       " 'shaheen',\n",
       " 'afridi',\n",
       " 'delivered',\n",
       " 'standout',\n",
       " 'performance',\n",
       " 'registering',\n",
       " 'impressive',\n",
       " 'figure',\n",
       " '4/35',\n",
       " 'afridi',\n",
       " 'end',\n",
       " 'inning',\n",
       " 'leading',\n",
       " 'wicket-taker',\n",
       " 'former',\n",
       " 'india',\n",
       " 'captain',\n",
       " 'batting',\n",
       " 'great',\n",
       " 'sunil',\n",
       " 'gavaskar',\n",
       " 'emphasised',\n",
       " 'naseem',\n",
       " 'shah',\n",
       " 'threat',\n",
       " 'side',\n",
       " 'prepares',\n",
       " 'face',\n",
       " 'pakistan',\n",
       " 'also',\n",
       " 'read',\n",
       " \"'india\",\n",
       " 'great',\n",
       " 'player',\n",
       " \"'m\",\n",
       " 'sorry',\n",
       " \"'re\",\n",
       " 'praising',\n",
       " 'u',\n",
       " 'much',\n",
       " 'pak',\n",
       " 'great',\n",
       " \"'s\",\n",
       " 'hard-hitting',\n",
       " 'remark',\n",
       " 'rohit',\n",
       " 'kohli',\n",
       " 'gavaskar',\n",
       " 'pointed',\n",
       " 'right-armer',\n",
       " 'terrific',\n",
       " 'opening',\n",
       " 'spell',\n",
       " 'created',\n",
       " 'problem',\n",
       " 'shubman',\n",
       " 'gill',\n",
       " 'insisting',\n",
       " 'lethal',\n",
       " 'out-swingers',\n",
       " 'made',\n",
       " 'difficult',\n",
       " 'indian',\n",
       " 'batter',\n",
       " 'start',\n",
       " '“',\n",
       " 'looked',\n",
       " '10',\n",
       " 'over',\n",
       " 'must',\n",
       " 'noticed',\n",
       " 'way',\n",
       " 'naseem',\n",
       " 'shah',\n",
       " 'bowled',\n",
       " 'out-swingers',\n",
       " 'brilliant',\n",
       " 'playing',\n",
       " 'pretty',\n",
       " 'difficult',\n",
       " 'shaheen',\n",
       " 'afridi',\n",
       " 'take',\n",
       " 'two',\n",
       " 'wicket',\n",
       " 'rohit',\n",
       " 'sharma',\n",
       " 'virat',\n",
       " 'kohli',\n",
       " 'focus',\n",
       " 'way',\n",
       " 'naseem',\n",
       " 'bowled',\n",
       " 'terrific',\n",
       " 'shubman',\n",
       " 'gill',\n",
       " 'playing',\n",
       " 'majorly',\n",
       " 'leaving',\n",
       " 'well',\n",
       " '”',\n",
       " 'gavaskar',\n",
       " 'said',\n",
       " 'joint',\n",
       " 'broadcast',\n",
       " 'sport',\n",
       " 'tak',\n",
       " 'samaa',\n",
       " 'tv',\n",
       " '“',\n",
       " 'batter',\n",
       " 'keep',\n",
       " 'getting',\n",
       " 'end',\n",
       " \"'s\",\n",
       " 'important',\n",
       " 'stick',\n",
       " 'around',\n",
       " 'shubman',\n",
       " \"n't\",\n",
       " 'best',\n",
       " 'form',\n",
       " 'right',\n",
       " 'know',\n",
       " 'take',\n",
       " 'responsibility',\n",
       " '50-over',\n",
       " 'game',\n",
       " 'shot',\n",
       " 'make',\n",
       " 'slow',\n",
       " 'start',\n",
       " 'guess',\n",
       " \"'s\",\n",
       " 'watchful',\n",
       " 'bit',\n",
       " 'nervous',\n",
       " 'naseem',\n",
       " 'shah',\n",
       " 'kept',\n",
       " 'shubman',\n",
       " 'gill',\n",
       " 'quiet',\n",
       " '”',\n",
       " 'said',\n",
       " 'gavaskar',\n",
       " 'rain',\n",
       " 'threat',\n",
       " 'loom',\n",
       " 'large',\n",
       " 'upcoming',\n",
       " 'india',\n",
       " 'v',\n",
       " 'pakistan',\n",
       " 'match',\n",
       " 'sunday',\n",
       " 'face',\n",
       " 'significant',\n",
       " 'risk',\n",
       " 'interrupted',\n",
       " 'rain',\n",
       " 'asian',\n",
       " 'cricket',\n",
       " 'council',\n",
       " 'triggered',\n",
       " 'controversy',\n",
       " 'earlier',\n",
       " 'week',\n",
       " 'designating',\n",
       " 'reserve',\n",
       " 'day',\n",
       " 'super',\n",
       " '4',\n",
       " 'encounter',\n",
       " 'two',\n",
       " 'arch-rivals',\n",
       " 'particularly',\n",
       " 'game',\n",
       " 'super',\n",
       " '4',\n",
       " 'allocated',\n",
       " 'reserve',\n",
       " 'day',\n",
       " 'benefit',\n",
       " 'decision',\n",
       " 'prompted',\n",
       " 'high',\n",
       " 'likelihood',\n",
       " 'rain',\n",
       " 'scheduled',\n",
       " 'match',\n",
       " 'day',\n",
       " 'sunday',\n",
       " 'however',\n",
       " 'weather',\n",
       " 'forecast',\n",
       " 'indicates',\n",
       " 'also',\n",
       " 'considerable',\n",
       " 'chance',\n",
       " 'rain',\n",
       " 'monday',\n",
       " 'mean',\n",
       " 'even',\n",
       " 'reserve',\n",
       " 'day',\n",
       " 'might',\n",
       " 'guarantee',\n",
       " 'uninterrupted',\n",
       " 'play']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens  # list of all the terms in the corpus, thos NOT the VOCAB, so there will be a lot of words repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0ba96c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 10), (4, 1), (5, 1), (6, 1), (7, 2), (8, 3), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 4), (21, 1), (22, 1), (23, 1), (24, 1), (25, 4), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 2), (33, 1), (34, 1), (35, 1), (36, 2), (37, 3), (38, 2), (39, 1), (40, 1), (41, 1), (42, 2), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 2), (56, 1), (57, 1), (58, 1), (59, 1), (60, 2), (61, 4), (62, 1), (63, 2), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 2), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 2), (77, 1), (78, 1), (79, 1), (80, 1), (81, 3), (82, 1), (83, 1), (84, 1), (85, 2), (86, 1), (87, 1), (88, 1), (89, 1), (90, 2), (91, 1), (92, 1), (93, 1), (94, 1), (95, 2), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 5), (104, 2), (105, 2), (106, 2), (107, 5), (108, 1), (109, 3), (110, 1), (111, 1), (112, 4), (113, 1), (114, 2), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 2), (123, 1), (124, 1), (125, 1), (126, 1), (127, 2), (128, 1), (129, 1), (130, 2), (131, 1), (132, 14), (133, 2), (134, 3), (135, 1), (136, 3), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 5), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 3), (161, 7), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 6), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 5), (175, 1), (176, 7), (177, 1), (178, 1), (179, 1), (180, 1), (181, 2), (182, 5), (183, 1), (184, 1), (185, 2), (186, 1), (187, 1), (188, 1), (189, 2), (190, 1), (191, 2), (192, 1), (193, 6), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 3), (200, 1), (201, 1), (202, 2), (203, 1), (204, 1), (205, 2), (206, 3), (207, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 3), (213, 6), (214, 1), (215, 1), (216, 1), (217, 4), (218, 1), (219, 1), (220, 1), (221, 2), (222, 1), (223, 1), (224, 1), (225, 2), (226, 5), (227, 1), (228, 1), (229, 1), (230, 1), (231, 1), (232, 1), (233, 2), (234, 1), (235, 1), (236, 1), (237, 1), (238, 3), (239, 1), (240, 1), (241, 1), (242, 1), (243, 1), (244, 4), (245, 1), (246, 1), (247, 1), (248, 3), (249, 1), (250, 1), (251, 2), (252, 1), (253, 1), (254, 1), (255, 2), (256, 1), (257, 1), (258, 3), (259, 1), (260, 4), (261, 3), (262, 2), (263, 1), (264, 1), (265, 4), (266, 3), (267, 2), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 1), (274, 3), (275, 1), (276, 2), (277, 2), (278, 1), (279, 1), (280, 1), (281, 4), (282, 1), (283, 3), (284, 1), (285, 1), (286, 1), (287, 2), (288, 3), (289, 1), (290, 2), (291, 1), (292, 1), (293, 2), (294, 1), (295, 1), (296, 1), (297, 1), (298, 1), (299, 1), (300, 1), (301, 1), (302, 1), (303, 4), (304, 1), (305, 1), (306, 1), (307, 4), (308, 1), (309, 2), (310, 1), (311, 3), (312, 1), (313, 1), (314, 2), (315, 1), (316, 2), (317, 1), (318, 1), (319, 3), (320, 1), (321, 1), (322, 1), (323, 1), (324, 2), (325, 1), (326, 1), (327, 6), (328, 6)]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of tokens into a bag of words corpus\n",
    "bow_corpus = [dictionary.doc2bow(tokens)]\n",
    "print(bow_corpus)   # (token_id, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e9ca6ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x14aff483688>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the LDA model\n",
    "lda_model = gensim.models.LdaModel(bow_corpus, num_topics=8, id2word=dictionary, passes=15)\n",
    "lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2aeea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "81f101d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: 0.003*\"india\" + 0.003*\"'s\" + 0.003*\"president\" + 0.003*\"match\" + 0.003*\"macron\" + 0.003*\"gavaskar\" + 0.003*\"kohli\" + 0.003*\"pakistan\" + 0.003*\"modi\" + 0.003*\"great\"\n",
      "\n",
      "Topic: 1\n",
      "Words: 0.025*\"india\" + 0.018*\"'s\" + 0.013*\"modi\" + 0.013*\"macron\" + 0.011*\"”\" + 0.011*\"“\" + 0.011*\"pakistan\" + 0.011*\"president\" + 0.011*\"match\" + 0.009*\"naseem\"\n",
      "\n",
      "Topic: 2\n",
      "Words: 0.003*\"india\" + 0.003*\"'s\" + 0.003*\"macron\" + 0.003*\"modi\" + 0.003*\"“\" + 0.003*\"match\" + 0.003*\"president\" + 0.003*\"french\" + 0.003*\"pakistan\" + 0.003*\"”\"\n",
      "\n",
      "Topic: 3\n",
      "Words: 0.003*\"india\" + 0.003*\"'s\" + 0.003*\"”\" + 0.003*\"“\" + 0.003*\"match\" + 0.003*\"gavaskar\" + 0.003*\"president\" + 0.003*\"modi\" + 0.003*\"naseem\" + 0.003*\"rain\"\n",
      "\n",
      "Topic: 4\n",
      "Words: 0.003*\"'s\" + 0.003*\"india\" + 0.003*\"modi\" + 0.003*\"“\" + 0.003*\"president\" + 0.003*\"minister\" + 0.003*\"match\" + 0.003*\"”\" + 0.003*\"macron\" + 0.003*\"pm\"\n",
      "\n",
      "Topic: 5\n",
      "Words: 0.003*\"india\" + 0.003*\"modi\" + 0.003*\"'s\" + 0.003*\"macron\" + 0.003*\"”\" + 0.003*\"pakistan\" + 0.003*\"gavaskar\" + 0.003*\"kohli\" + 0.003*\"“\" + 0.003*\"french\"\n",
      "\n",
      "Topic: 6\n",
      "Words: 0.003*\"india\" + 0.003*\"'s\" + 0.003*\"naseem\" + 0.003*\"”\" + 0.003*\"minister\" + 0.003*\"macron\" + 0.003*\"match\" + 0.003*\"“\" + 0.003*\"rain\" + 0.003*\"president\"\n",
      "\n",
      "Topic: 7\n",
      "Words: 0.003*\"india\" + 0.003*\"pakistan\" + 0.003*\"macron\" + 0.003*\"match\" + 0.003*\"modi\" + 0.003*\"'s\" + 0.003*\"president\" + 0.003*\"”\" + 0.003*\"“\" + 0.003*\"rain\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx}\\nWords: {topic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54e972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "de9c346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4d120e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6243bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [preprocess(paragraph) for paragraph in text.split(\"\\n\") if paragraph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8a28dba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f04641e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pm',\n",
       "  'modi',\n",
       "  'macron',\n",
       "  'hold',\n",
       "  'lunch',\n",
       "  'meet',\n",
       "  'eye',\n",
       "  'india-france',\n",
       "  'tie',\n",
       "  'new',\n",
       "  'height',\n",
       "  'progress'],\n",
       " ['french',\n",
       "  'president',\n",
       "  'emmanuel',\n",
       "  'macron',\n",
       "  'said',\n",
       "  'india',\n",
       "  'utmost',\n",
       "  'g-20',\n",
       "  'presidency',\n",
       "  'serve',\n",
       "  'unity',\n",
       "  'peace'],\n",
       " ['prime',\n",
       "  'minister',\n",
       "  'narendra',\n",
       "  'modi',\n",
       "  'sunday',\n",
       "  'lunch',\n",
       "  'french',\n",
       "  'president',\n",
       "  'emmanuel',\n",
       "  'macron',\n",
       "  'following',\n",
       "  'conclusion',\n",
       "  'g20',\n",
       "  'summit'],\n",
       " ['prime',\n",
       "  'minister',\n",
       "  'narendra',\n",
       "  'modi',\n",
       "  'french',\n",
       "  'president',\n",
       "  'emmanuel',\n",
       "  'macron',\n",
       "  'x/narendra',\n",
       "  'modi'],\n",
       " ['“',\n",
       "  'productive',\n",
       "  'lunch',\n",
       "  'meeting',\n",
       "  'president',\n",
       "  'emmanuelmacron',\n",
       "  'discussed',\n",
       "  'series',\n",
       "  'topic',\n",
       "  'look',\n",
       "  'forward',\n",
       "  'ensuring',\n",
       "  'india-france',\n",
       "  'relation',\n",
       "  'scale',\n",
       "  'new',\n",
       "  'height',\n",
       "  'progress',\n",
       "  '”',\n",
       "  'pm',\n",
       "  'modi',\n",
       "  'posted',\n",
       "  'social',\n",
       "  'medium',\n",
       "  'platform',\n",
       "  'x',\n",
       "  'formerly',\n",
       "  'twitter'],\n",
       " ['prime',\n",
       "  'minister',\n",
       "  'accompanied',\n",
       "  'external',\n",
       "  'affair',\n",
       "  'minister',\n",
       "  'jaishankar',\n",
       "  'national',\n",
       "  'security',\n",
       "  'advisor',\n",
       "  'ajit',\n",
       "  'doval'],\n",
       " ['“',\n",
       "  'develop',\n",
       "  'defence',\n",
       "  'cooperation',\n",
       "  'india',\n",
       "  '”',\n",
       "  'macron',\n",
       "  'quoted',\n",
       "  'pti',\n",
       "  'saying'],\n",
       " ['earlier',\n",
       "  'macron',\n",
       "  'thanked',\n",
       "  'prime',\n",
       "  'minister',\n",
       "  'modi',\n",
       "  'hailed',\n",
       "  'india',\n",
       "  \"'s\",\n",
       "  'effort',\n",
       "  'g20',\n",
       "  'presidency',\n",
       "  'serve',\n",
       "  'peace',\n",
       "  'unity',\n",
       "  '“',\n",
       "  'thank',\n",
       "  'pm',\n",
       "  'modi',\n",
       "  'faithful',\n",
       "  'principle',\n",
       "  'india',\n",
       "  'utmost',\n",
       "  'g-20',\n",
       "  'presidency',\n",
       "  'serve',\n",
       "  'unity',\n",
       "  'peace',\n",
       "  'send',\n",
       "  'across',\n",
       "  'message',\n",
       "  'unity',\n",
       "  'russia',\n",
       "  'still',\n",
       "  'waging',\n",
       "  'aggression',\n",
       "  'ukraine',\n",
       "  '”',\n",
       "  'ani',\n",
       "  'quoted',\n",
       "  'french',\n",
       "  'president',\n",
       "  'saying'],\n",
       " ['macron',\n",
       "  'also',\n",
       "  'called',\n",
       "  'deep',\n",
       "  'reform',\n",
       "  'international',\n",
       "  'organisation',\n",
       "  '“',\n",
       "  'support',\n",
       "  'deep',\n",
       "  'reform',\n",
       "  'global',\n",
       "  'governance',\n",
       "  'security',\n",
       "  'council',\n",
       "  'well',\n",
       "  'world',\n",
       "  'bank',\n",
       "  'imf',\n",
       "  'reflect',\n",
       "  'today',\n",
       "  \"'s\",\n",
       "  'reality',\n",
       "  'term',\n",
       "  'demography',\n",
       "  'economy',\n",
       "  'well',\n",
       "  'want',\n",
       "  'increase',\n",
       "  'available',\n",
       "  'tool',\n",
       "  \"'s\",\n",
       "  'want',\n",
       "  'replenish',\n",
       "  'world',\n",
       "  'bank',\n",
       "  'france',\n",
       "  'supporting',\n",
       "  'emerging',\n",
       "  'country',\n",
       "  'greater',\n",
       "  'role',\n",
       "  'play',\n",
       "  '”',\n",
       "  'french',\n",
       "  'president',\n",
       "  'said.shaheen',\n",
       "  'dismissed',\n",
       "  'rohit',\n",
       "  'kohli',\n",
       "  'focus',\n",
       "  '...',\n",
       "  'gavaskar',\n",
       "  'alert',\n",
       "  'team',\n",
       "  'india',\n",
       "  'another',\n",
       "  'pakistan',\n",
       "  'star'],\n",
       " ['india',\n",
       "  'batting',\n",
       "  'great',\n",
       "  'spoke',\n",
       "  'detail',\n",
       "  'previous',\n",
       "  'match',\n",
       "  'two',\n",
       "  'side',\n",
       "  'group',\n",
       "  'stage'],\n",
       " ['team',\n",
       "  'india',\n",
       "  'preparing',\n",
       "  'highly',\n",
       "  'anticipated',\n",
       "  'showdown',\n",
       "  'pakistan',\n",
       "  'asia',\n",
       "  'cup',\n",
       "  'super',\n",
       "  '4',\n",
       "  'stage',\n",
       "  'sunday',\n",
       "  'two',\n",
       "  'side',\n",
       "  'previously',\n",
       "  'crossed',\n",
       "  'path',\n",
       "  'group',\n",
       "  'stage',\n",
       "  'match',\n",
       "  'abandoned',\n",
       "  'due',\n",
       "  'rain',\n",
       "  'pallekele',\n",
       "  'india',\n",
       "  'played',\n",
       "  'whole',\n",
       "  'inning',\n",
       "  'scoring',\n",
       "  '266',\n",
       "  'match',\n",
       "  'leave',\n",
       "  'indian',\n",
       "  'team',\n",
       "  'management',\n",
       "  'wit',\n",
       "  'significant',\n",
       "  'concern',\n",
       "  'top-order',\n",
       "  'failure',\n",
       "  'india',\n",
       "  'left',\n",
       "  'reeling',\n",
       "  '66/4',\n",
       "  'rohit',\n",
       "  'sharma',\n",
       "  'virat',\n",
       "  'kohli',\n",
       "  'castled',\n",
       "  'left-armer',\n",
       "  'shaheen',\n",
       "  'afridi',\n",
       "  'fellow',\n",
       "  'pacer',\n",
       "  'naseem',\n",
       "  'shah',\n",
       "  'haris',\n",
       "  'rauf',\n",
       "  'also',\n",
       "  'making',\n",
       "  'life',\n",
       "  'miserable',\n",
       "  'indian',\n",
       "  'batter'],\n",
       " ['india',\n",
       "  \"'s\",\n",
       "  'virat',\n",
       "  'kohli',\n",
       "  'walk',\n",
       "  'back',\n",
       "  'pavilion',\n",
       "  'dismissal',\n",
       "  'asia',\n",
       "  'cup',\n",
       "  '2023',\n",
       "  'match',\n",
       "  'india',\n",
       "  'pakistan',\n",
       "  'afp'],\n",
       " ['however',\n",
       "  'hardik',\n",
       "  'pandya',\n",
       "  \"'s\",\n",
       "  'resilient',\n",
       "  'inning',\n",
       "  '87',\n",
       "  'run',\n",
       "  'ishan',\n",
       "  'kishan',\n",
       "  \"'s\",\n",
       "  'impressive',\n",
       "  '82',\n",
       "  'helped',\n",
       "  'india',\n",
       "  'recover',\n",
       "  'early',\n",
       "  'setback',\n",
       "  'guiding',\n",
       "  'total',\n",
       "  '266',\n",
       "  'among',\n",
       "  'pakistan',\n",
       "  \"'s\",\n",
       "  'bowler',\n",
       "  'star',\n",
       "  'pacer',\n",
       "  'shaheen',\n",
       "  'afridi',\n",
       "  'delivered',\n",
       "  'standout',\n",
       "  'performance',\n",
       "  'registering',\n",
       "  'impressive',\n",
       "  'figure',\n",
       "  '4/35',\n",
       "  'afridi',\n",
       "  'end',\n",
       "  'inning',\n",
       "  'leading',\n",
       "  'wicket-taker',\n",
       "  'former',\n",
       "  'india',\n",
       "  'captain',\n",
       "  'batting',\n",
       "  'great',\n",
       "  'sunil',\n",
       "  'gavaskar',\n",
       "  'emphasised',\n",
       "  'naseem',\n",
       "  'shah',\n",
       "  'threat',\n",
       "  'side',\n",
       "  'prepares',\n",
       "  'face',\n",
       "  'pakistan'],\n",
       " ['also',\n",
       "  'read',\n",
       "  \"'india\",\n",
       "  'great',\n",
       "  'player',\n",
       "  \"'m\",\n",
       "  'sorry',\n",
       "  \"'re\",\n",
       "  'praising',\n",
       "  'u',\n",
       "  'much',\n",
       "  'pak',\n",
       "  'great',\n",
       "  \"'s\",\n",
       "  'hard-hitting',\n",
       "  'remark',\n",
       "  'rohit',\n",
       "  'kohli'],\n",
       " ['gavaskar',\n",
       "  'pointed',\n",
       "  'right-armer',\n",
       "  'terrific',\n",
       "  'opening',\n",
       "  'spell',\n",
       "  'created',\n",
       "  'problem',\n",
       "  'shubman',\n",
       "  'gill',\n",
       "  'insisting',\n",
       "  'lethal',\n",
       "  'out-swingers',\n",
       "  'made',\n",
       "  'difficult',\n",
       "  'indian',\n",
       "  'batter',\n",
       "  'start'],\n",
       " ['“',\n",
       "  'looked',\n",
       "  '10',\n",
       "  'over',\n",
       "  'must',\n",
       "  'noticed',\n",
       "  'way',\n",
       "  'naseem',\n",
       "  'shah',\n",
       "  'bowled',\n",
       "  'out-swingers',\n",
       "  'brilliant',\n",
       "  'playing',\n",
       "  'pretty',\n",
       "  'difficult',\n",
       "  'shaheen',\n",
       "  'afridi',\n",
       "  'take',\n",
       "  'two',\n",
       "  'wicket',\n",
       "  'rohit',\n",
       "  'sharma',\n",
       "  'virat',\n",
       "  'kohli',\n",
       "  'focus',\n",
       "  'way',\n",
       "  'naseem',\n",
       "  'bowled',\n",
       "  'terrific',\n",
       "  'shubman',\n",
       "  'gill',\n",
       "  'playing',\n",
       "  'majorly',\n",
       "  'leaving',\n",
       "  'well',\n",
       "  '”',\n",
       "  'gavaskar',\n",
       "  'said',\n",
       "  'joint',\n",
       "  'broadcast',\n",
       "  'sport',\n",
       "  'tak',\n",
       "  'samaa',\n",
       "  'tv'],\n",
       " ['“',\n",
       "  'batter',\n",
       "  'keep',\n",
       "  'getting',\n",
       "  'end',\n",
       "  \"'s\",\n",
       "  'important',\n",
       "  'stick',\n",
       "  'around',\n",
       "  'shubman',\n",
       "  \"n't\",\n",
       "  'best',\n",
       "  'form',\n",
       "  'right',\n",
       "  'know',\n",
       "  'take',\n",
       "  'responsibility',\n",
       "  '50-over',\n",
       "  'game',\n",
       "  'shot',\n",
       "  'make',\n",
       "  'slow',\n",
       "  'start',\n",
       "  'guess',\n",
       "  \"'s\",\n",
       "  'watchful',\n",
       "  'bit',\n",
       "  'nervous',\n",
       "  'naseem',\n",
       "  'shah',\n",
       "  'kept',\n",
       "  'shubman',\n",
       "  'gill',\n",
       "  'quiet',\n",
       "  '”',\n",
       "  'said',\n",
       "  'gavaskar'],\n",
       " ['rain', 'threat', 'loom', 'large'],\n",
       " ['upcoming',\n",
       "  'india',\n",
       "  'v',\n",
       "  'pakistan',\n",
       "  'match',\n",
       "  'sunday',\n",
       "  'face',\n",
       "  'significant',\n",
       "  'risk',\n",
       "  'interrupted',\n",
       "  'rain',\n",
       "  'asian',\n",
       "  'cricket',\n",
       "  'council',\n",
       "  'triggered',\n",
       "  'controversy',\n",
       "  'earlier',\n",
       "  'week',\n",
       "  'designating',\n",
       "  'reserve',\n",
       "  'day',\n",
       "  'super',\n",
       "  '4',\n",
       "  'encounter',\n",
       "  'two',\n",
       "  'arch-rivals',\n",
       "  'particularly',\n",
       "  'game',\n",
       "  'super',\n",
       "  '4',\n",
       "  'allocated',\n",
       "  'reserve',\n",
       "  'day',\n",
       "  'benefit'],\n",
       " ['decision',\n",
       "  'prompted',\n",
       "  'high',\n",
       "  'likelihood',\n",
       "  'rain',\n",
       "  'scheduled',\n",
       "  'match',\n",
       "  'day',\n",
       "  'sunday',\n",
       "  'however',\n",
       "  'weather',\n",
       "  'forecast',\n",
       "  'indicates',\n",
       "  'also',\n",
       "  'considerable',\n",
       "  'chance',\n",
       "  'rain',\n",
       "  'monday',\n",
       "  'mean',\n",
       "  'even',\n",
       "  'reserve',\n",
       "  'day',\n",
       "  'might',\n",
       "  'guarantee',\n",
       "  'uninterrupted',\n",
       "  'play']]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "87dcdae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tokens)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "93b58486",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "lda_model = gensim.models.LdaModel(bow_corpus, num_topics=2, iterations=100 , \\\n",
    "                                   id2word=dictionary, passes=25, alpha='auto', eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "950e60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "afae09d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: 0.130*\"modi\" + 0.130*\"macron\" + 0.113*\"president\" + 0.113*\"“\" + 0.113*\"”\" + 0.095*\"french\" + 0.088*\"'s\" + 0.086*\"india\" + 0.078*\"gavaskar\" + 0.035*\"kohli\"\n",
      "\n",
      "Topic: 1\n",
      "Words: 0.268*\"india\" + 0.174*\"match\" + 0.171*\"pakistan\" + 0.158*\"'s\" + 0.107*\"kohli\" + 0.040*\"gavaskar\" + 0.014*\"french\" + 0.014*\"”\" + 0.014*\"“\" + 0.014*\"president\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx}\\nWords: {topic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "d0dffe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.4243600442768668\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=tokens, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c822ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "eba943e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract the top 10 terms for each topic\n",
    "topics = lda_model.show_topics(formatted=False, num_topics=2, num_words=10)\n",
    "\n",
    "# Initialize empty lists to hold topic names and terms\n",
    "topic_names = []\n",
    "topic_terms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "e766e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the lists\n",
    "for idx, topic in topics:\n",
    "    topic_names.append(f\"Topic {idx}\")  # Change this to provide more meaningful names if needed\n",
    "    terms = \", \".join([word[0] for word in topic])\n",
    "    topic_terms.append(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b6f8ed81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic Name</th>\n",
       "      <th>Top 10 Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topic 0</td>\n",
       "      <td>modi, macron, president, “, ”, french, 's, ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topic 1</td>\n",
       "      <td>india, match, pakistan, 's, kohli, gavaskar, f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic Name                                       Top 10 Terms\n",
       "0    Topic 0  modi, macron, president, “, ”, french, 's, ind...\n",
       "1    Topic 1  india, match, pakistan, 's, kohli, gavaskar, f..."
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Topic Name': topic_names,\n",
    "    'Top 10 Terms': topic_terms\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 0,1,3 are same\n",
    "# topics 2 & 4 are similaar \n",
    "\n",
    "# hence the number of meaningful topics in this dataset is still 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fbbc5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Export the table to a CSV for sharing/presentation\n",
    "df.to_csv('topics_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b0757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02689c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd2674a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
